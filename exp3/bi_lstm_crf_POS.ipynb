{"cells":[{"cell_type":"markdown","metadata":{},"source":["# 安装包"]},{"cell_type":"markdown","metadata":{},"source":["## 1）安装keras-preprocessing"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["!pip install keras-preprocessing==1.0.9"]},{"cell_type":"markdown","metadata":{},"source":["## 2）解压keras-contrib-master.zip"]},{"cell_type":"markdown","metadata":{"trusted":true},"source":["!unzip work/keras-contrib-master.zip"]},{"cell_type":"markdown","metadata":{},"source":["## 3）安装keras-contrib-master "]},{"cell_type":"markdown","metadata":{"scrolled":true,"trusted":true},"source":["!python keras-contrib-master/setup.py install"]},{"cell_type":"markdown","metadata":{},"source":["## 4）解压bi-lstm-crf-master.zip"]},{"cell_type":"markdown","metadata":{"scrolled":true,"trusted":true},"source":["!unzip work/bi-lstm-crf-master.zip"]},{"cell_type":"markdown","metadata":{},"source":["## 5）安装bi-lstm-crf "]},{"cell_type":"markdown","metadata":{"scrolled":true,"trusted":true},"source":["!python bi-lstm-crf-master/setup.py install"]},{"cell_type":"markdown","metadata":{},"source":["# 配置环境，并导入包"]},{"cell_type":"markdown","metadata":{},"source":["### 如果上面的步骤执行成功后，该步骤提示模块dl_segmenter不存在，请重启kernel后再执行这一步。重启kernel的方法参考实验手册"]},{"cell_type":"code","execution_count":1,"metadata":{"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["Using TensorFlow backend.\n"]}],"source":["#配置环境，并导入包\n","import sys\n","path1 = \"./tookit/bi-lstm-crf-master\"\n","sys.path.append(path1)\n","path2 = \"./tookit/keras-contrib-master\"\n","sys.path.append(path2)\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from keras.optimizers import Adam\n","from dl_segmenter import get_or_create, save_config,DLSegmenter\n","from dl_segmenter.custom.callbacks import LRFinder, SGDRScheduler, WatchScheduler\n","from dl_segmenter.data_loader import DataLoader\n","from dl_segmenter.utils import make_dictionaries\n","import os\n","import re"]},{"cell_type":"markdown","metadata":{},"source":["# 解压数据"]},{"cell_type":"markdown","metadata":{"trusted":false},"source":["!unzip work/people-2014.zip"]},{"cell_type":"markdown","metadata":{},"source":["# 数据预处理"]},{"cell_type":"markdown","metadata":{},"source":["## 1.将标注的语料转化成BIS形式，并合并在一个文件中"]},{"cell_type":"code","execution_count":8,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["开始转化...\n","bi-lstm-crf-master/data/2014_processed\n","bi-lstm-crf-master/data/2014_processed/0101\n","bi-lstm-crf-master/data/2014_processed/0102\n","bi-lstm-crf-master/data/2014_processed/0103\n","bi-lstm-crf-master/data/2014_processed/0104\n","bi-lstm-crf-master/data/2014_processed/0105\n","bi-lstm-crf-master/data/2014_processed/0106\n","bi-lstm-crf-master/data/2014_processed/0107\n","bi-lstm-crf-master/data/2014_processed/0108\n","bi-lstm-crf-master/data/2014_processed/0109\n","bi-lstm-crf-master/data/2014_processed/0110\n","bi-lstm-crf-master/data/2014_processed/0111\n","bi-lstm-crf-master/data/2014_processed/0112\n","bi-lstm-crf-master/data/2014_processed/0113\n","bi-lstm-crf-master/data/2014_processed/0114\n","bi-lstm-crf-master/data/2014_processed/0115\n","bi-lstm-crf-master/data/2014_processed/0116\n","bi-lstm-crf-master/data/2014_processed/0117\n","bi-lstm-crf-master/data/2014_processed/0118\n","bi-lstm-crf-master/data/2014_processed/0119\n","bi-lstm-crf-master/data/2014_processed/0120\n","bi-lstm-crf-master/data/2014_processed/0121\n","bi-lstm-crf-master/data/2014_processed/0122\n","bi-lstm-crf-master/data/2014_processed/0123\n","转化完成\n"]}],"source":["#B:表示语句块的开始，I:表示非语句块的开始，S:表示单独成词\n","#示例如下：\n","# 华尔街/nsf 股市/n 。/w   ->    华 尔 街 股 市 。  B-nsf I-nsf I-nsf B-n I-n S-w\n","def print_process(process):\n","    num_processed = int(30 * process)\n","    num_unprocessed = 30 - num_processed\n","    print(\n","        f\"{''.join(['['] + ['='] * num_processed + ['>'] + [' '] * num_unprocessed + [']'])}, {(process * 100):.2f} %\")\n","\n","\n","def convert_to_bis(source_dir, target_path, log=False, combine=False, single_line=True):\n","    print(\"开始转化...\")\n","    for root, dirs, files in os.walk(source_dir):\n","        total = len(files)\n","        tgt_dir = target_path + root[len(source_dir):]\n","\n","        print(tgt_dir)\n","        for index, name in enumerate(files):\n","            file = os.path.join(root, name)\n","            bises = process_file(file)\n","            if combine:\n","                _save_bises(bises, target_path, write_mode='a', single_line=single_line)\n","            else:\n","                os.makedirs(tgt_dir, exist_ok=True)\n","                _save_bises(bises, os.path.join(tgt_dir, name), single_line=single_line)\n","            if log:\n","                print_process((index + 1) / total)\n","    print(\"转化完成\")\n","\n","\n","def _save_bises(bises, path, write_mode='w+', single_line=True):\n","    with open(path, mode=write_mode, encoding='UTF-8') as f:\n","        if single_line:\n","            for bis in bises:\n","                sent, tags = [], []\n","                for char, tag in bis:\n","                    sent.append(char)\n","                    tags.append(tag)\n","                sent = ' '.join(sent)\n","                tags = ' '.join(tags)\n","                f.write(sent + \"\\t\" + tags)\n","                f.write('\\n')\n","        else:\n","            for bis in bises:\n","                for char, tag in bis:\n","                    f.write(char + \"\\t\" + tag + \"\\n\")\n","                f.write(\"\\n\")\n","\n","\n","def process_file(file):\n","    with open(file, 'r', encoding='UTF-8') as f:\n","        text = f.readlines()\n","        bises = _parse_text(text)\n","    return bises\n","\n","\n","def _parse_text(text: list):\n","    bises = []\n","    for line in text:\n","        # remove POS tag\n","        line, _ = re.subn('\\\\n', '', line)\n","        if line == '' or line == '\\n':\n","            continue\n","        words = re.split('\\s+', line)\n","\n","        if len(words) > MAX_LEN_SIZE:\n","            texts = re.split('[。？！，.?!,]/w', line)\n","            if len(min(texts, key=len)) > MAX_LEN_SIZE:\n","                continue\n","            bises.extend(_parse_text(texts))\n","        else:\n","            bises.append(_tag(words))\n","    return bises\n","\n","#给指定的一行文本打上BIS标签\n","def _tag(words): \n","    bis = []\n","    # words = list(map(list, words))\n","    pre_word = None\n","    for word in words:\n","        pos_t = None\n","        tokens = word.split('/')\n","        if len(tokens) == 2:\n","            word, pos = tokens\n","        elif len(tokens) == 3:\n","            word, pos_t, pos = tokens\n","        else:\n","            continue\n","\n","        word = list(word)\n","        pos = pos.upper()\n","\n","        if len(word) == 0:\n","            continue\n","        if word[0] == '[':\n","            pre_word = word\n","            continue\n","        if pre_word is not None:\n","            pre_word += word\n","            if pos_t is None:\n","                continue\n","            elif pos_t[-1] != ']':\n","                continue\n","            else:\n","                word = pre_word[1:]\n","                pre_word = None\n","\n","        if len(word) == 1:\n","            bis.append((word[0], 'S-' + pos))\n","        else:\n","            for i, char in enumerate(word):\n","                if i == 0:\n","                    bis.append((char, 'B-' + pos))\n","                else:\n","                    bis.append((char, 'I-' + pos))\n","    # bis.append(('\\n', 'O'))\n","    return bis\n","\n","corups_dir=\"./tookit/people-2014/train\" #指定存放语料库的文件夹，程序将会递归查找目录下的文件。\n","output_path=\"./tookit/bi-lstm-crf-master/data/2014_processed\"  #指定标记好的文件的输出路径。\n","combine = True\n","MAX_LEN_SIZE =  150   #处理后的最大语句长度（将原句子按标点符号断句，若断句后的长度仍比最大长度长，将忽略\n","\n","log = False\n","convert_to_bis(corups_dir, output_path, log, combine)"]},{"cell_type":"markdown","metadata":{},"source":["## 2.生成字典"]},{"cell_type":"code","execution_count":9,"metadata":{"scrolled":true,"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["开始生成...\n","生成字典结束.\n"]}],"source":["file_path = \"./tookit/bi-lstm-crf-master/data/2014_processed\" #用于生成字典的标注文件\n","src_dict_path = \"./tookit/bi-lstm-crf-master/config/src_dict.json\" #源字典保存路径\n","tgt_dict_path = \"./tookit/bi-lstm-crf-master/config/tgt_dict.json\" #目标字典保存路径\n","min_freq = 1 #词频数阈值，小于该阈值的词将被忽略\n","print(\"开始生成...\")\n","make_dictionaries(file_path,\n","                  src_dict_path=src_dict_path,\n","                  tgt_dict_path=tgt_dict_path,\n","                  filters=\"\\t\\n\",\n","                  oov_token=\"<UNK>\",\n","                  min_freq=min_freq)\n","print(\"生成字典结束.\")"]},{"cell_type":"markdown","metadata":{},"source":["## 3.转化成h5文件"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":false},"outputs":[{"name":"stdout","output_type":"stream","text":["开始转化...\n","转化完成.\n"]}],"source":["#可将文本文件2014_processed转换为hdf5格式，提升训练速度\n","\n","src_dict_path = \"./tookit/bi-lstm-crf-master/config/src_dict.json\" #源字典保存路径\n","tgt_dict_path = \"./tookit/bi-lstm-crf-master/config/tgt_dict.json\" #目标字典保存路径\n","txt_path =\"./tookit/bi-lstm-crf-master/data/2014_processed\" #BIS标注的文本文件路径\n","h5_path = \"./tookit/bi-lstm-crf-master/data/2014_processed.h5\" #转换为hdf5格式的保存路径\n","seq_len = 150  #语句长度\n","data_loader = DataLoader(src_dict_path, tgt_dict_path,\n","                             batch_size=1,\n","                             max_len=seq_len,\n","                             sparse_target=False)\n","print(\"开始转化...\")\n","data_loader.load_and_dump_to_h5(txt_path, h5_path, encoding='utf-8')\n","print(\"转化完成.\")"]},{"cell_type":"markdown","metadata":{},"source":["# 配置相关参数"]},{"cell_type":"code","execution_count":14,"metadata":{"scrolled":true,"trusted":false},"outputs":[],"source":["h5_dataset_path = \"./tookit/bi-lstm-crf-master/data/2014_processed.h5\"  # 转换为hdf5格式的数据集\n","config_save_path = \"./tookit/bi-lstm-crf-master/config/default-config.json\"  # 模型配置路径\n","weights_save_path = \"./tookit/bi-lstm-crf-master/models/weights.{epoch:02d}-{val_loss:.2f}.h5\"  # 模型权重保存路径\n","init_weights_path = \"./tookit/bi-lstm-crf-master/models/weights.32-0.18.h5\"  # 预训练模型权重文件路径\n","#embedding_file_path = \"../data/sgns.renmin.word\"  # 词向量文件路径，若不使用设为None\n","embedding_file_path = None  # 词向量文件路径，若不使用设为None\n","\n","src_dict_path = \"./tookit/bi-lstm-crf-master/config/src_dict.json\"  # 源字典路径\n","tgt_dict_path = \"./tookit/bi-lstm-crf-master/config/tgt_dict.json\"  # 目标字典路径\n","batch_size = 32\n","epochs = 10\n","\n","\n","\n","# GPU 下用于选择训练的GPU\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n","\n","data_loader = DataLoader(src_dict_path=src_dict_path,\n","                         tgt_dict_path=tgt_dict_path,\n","                         batch_size=batch_size)\n","\n","steps_per_epoch = 2000\n","validation_steps = 20\n","\n","config = {\n","    \"vocab_size\": data_loader.src_vocab_size,\n","    \"chunk_size\": data_loader.tgt_vocab_size,\n","    \"embed_dim\": 300,\n","    \"bi_lstm_units\": 256,\n","     \"max_num_words\": 20000,\n","    \"dropout_rate\": 0.1\n","}\n"]},{"cell_type":"markdown","metadata":{},"source":["# 加载数据"]},{"cell_type":"code","execution_count":12,"metadata":{"trusted":false},"outputs":[],"source":["#加载数据,并将数据分割成训练集合验证集\n","X_train, Y_train, X_valid, Y_valid = DataLoader.load_data(h5_dataset_path, frac=0.8)"]},{"cell_type":"markdown","metadata":{},"source":["# 定义并训练模型"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"trusted":false},"outputs":[],"source":["#定义模型\n","tokenizer = get_or_create(config,\n","                          optimizer=Adam(),\n","                          embedding_file=embedding_file_path,\n","                          src_dict_path=src_dict_path,\n","                          weights_path=init_weights_path)\n","print(tokenizer)\n","save_config(tokenizer, config_save_path)\n","\n","#ModelCheckpoint 保存最佳模型\n","ck = ModelCheckpoint(weights_save_path,\n","                     save_best_only=True,\n","                     save_weights_only=True,\n","                     monitor='val_loss',\n","                     verbose=0)\n","#创建日志\n","log = TensorBoard(log_dir='./tookit/bi-lstm-crf-master/logs',\n","                  histogram_freq=0,\n","                  batch_size=data_loader.batch_size,\n","                  write_graph=True,\n","                  write_grads=False)\n","\n","# 使用LRFinder寻找有效的学习率\n","lr_finder = LRFinder(1e-6, 1e-2, steps_per_epoch, epochs=1)  # => (2e-4, 3e-4)\n","lr_scheduler = WatchScheduler(lambda _, lr: lr / 2, min_lr=2e-4, max_lr=4e-4, watch=\"val_loss\", watch_his_len=2)\n","lr_scheduler = SGDRScheduler(min_lr=4e-5, max_lr=1e-3, steps_per_epoch=steps_per_epoch,\n","                             cycle_length=15,\n","                             lr_decay=0.9,\n","                             mult_factor=1.2)\n","\n","\n","#训练模型\n","tokenizer.model.fit_generator(data_loader.generator_from_data(X_train, Y_train),\n","                              epochs=1,\n","                              steps_per_epoch=steps_per_epoch,\n","                              validation_data=data_loader.generator_from_data(X_valid, Y_valid),\n","                              validation_steps=validation_steps,\n","                              callbacks=[ck, log, lr_finder])\n","#画出损失函数\n","lr_finder.plot_loss()\n"]},{"cell_type":"markdown","metadata":{},"source":["# 词性标注测试"]},{"cell_type":"markdown","metadata":{},"source":["## 1) 重启kernel"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["请按照实验手册上面的操作重启kernel"]},{"cell_type":"markdown","metadata":{},"source":["## 2）导入包"]},{"cell_type":"code","execution_count":3,"metadata":{"trusted":true},"outputs":[],"source":["#配置环境，并导入包\n","import sys\n","path1 = \"./tookit/bi-lstm-crf-master\"\n","sys.path.append(path1)\n","path2 = \"./tookit/keras-contrib-master\"\n","sys.path.append(path2)\n","from keras.callbacks import ModelCheckpoint, TensorBoard\n","from keras.optimizers import Adam\n","from dl_segmenter import get_or_create, save_config,DLSegmenter\n","from dl_segmenter.custom.callbacks import LRFinder, SGDRScheduler, WatchScheduler\n","from dl_segmenter.data_loader import DataLoader\n","from dl_segmenter.utils import make_dictionaries\n","import os"]},{"cell_type":"code","execution_count":4,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["('华为', 'a') ('是', 'vshi') ('全', 'a') ('球', 'q') ('领先', 'vi') ('的', 'ude1') ('I', 'ng') ('C', 'x') ('T', 'w') ('（', 'w') ('信', 'n') ('息', 'c') ('与', 'qv') ('通信', 'vn') ('）', 'w') ('基础', 'n') ('设施', 'n') ('和', 'cc') ('智', 'n') ('能', 'v') ('终', 'd') ('端', 'v') ('提', 'v') ('供', 'x') ('商', 'w') ('，', 'w') ('致力于', 'v') ('把', 'pba') ('数字', 'n') ('世界', 'vi') ('带入', 'v') ('每', 'n') ('个人', 'n') ('、', 'w') ('每', 'n') ('个', 'q') ('家庭', 'nz') ('、', 'w') ('每', 'n') ('个', 'q') ('组织', 'n') ('，', 'w') ('构', 'd') ('建', 'v') ('万', 'w') ('物互', 'nr') ('联', 'ng') ('的', 'ude1') ('智', 'n') ('能', 'v') ('世界', 'vi') ('。', 'w') ('我们', 'rr') ('在', 'p') ('通信', 'vn') ('网络', 'n') ('、', 'w') ('I', 'ng') ('T', 'w') ('、', 'w') ('智', 'n') ('能', 'v') ('终', 'd') ('端', 'v') ('和', 'cc') ('云', 'b') ('服务', 'vn') ('等', 'udeng') ('领域', 'vi') ('为', 'p') ('客户', 'n') ('提', 'v') ('供', 'x') ('有', 'vyou') ('竞', 'bl') ('争', 'v') ('力', 'n') ('、', 'w') ('安全', 'an') ('可信赖', 'nz') ('的', 'ude1') ('产品', 'n') ('、', 'w') ('解决方案与', 'gi') ('服务', 'vn') ('，', 'w') ('与', 'qv') ('生态伙', 'nz') ('伴', 'c') ('开放', 'v') ('合作', 'vn') ('，持', 'ntu') ('续', 'vd') ('为', 'p') ('客户', 'n') ('创造价值', 'nz') ('，', 'w') ('释', 'vg') ('放', 'v') ('个人潜', 'n') ('能', 'v') ('，', 'w') ('丰富', 'ns') ('家庭生活', 'nz') ('，', 'w') ('激发', 'v') ('组织', 'n') ('创新', 'vi') ('。', 'w') ('华为坚', 'a') ('持围', 'v') ('绕客户', 'n') ('需求', 'n') ('持续创新', 'nz') ('，', 'w') ('加大', 'v') ('基', 'ng') ('础', 'ag') ('研究', 'vn') ('投入', 'v') ('，', 'w') ('厚积薄发', 'nz') ('，', 'w') ('推动', 'v') ('世界进步', 'nz') ('。', 'w') ('华为', 'a') ('成立', 'vi') ('于', 'p') ('1987年', 't') ('，', 'w') ('是', 'vshi') ('一家', 'mq') ('由', 'p') ('员工', 'n') ('持有', 'v') ('全部', 'm') ('股', 'c') ('份', 'q') ('的', 'ude1') ('民', 'ng') ('营', 'qt') ('企', 'ng') ('业', 'm') ('，', 'w') ('目前', 't') ('有', 'vyou') ('1', 'm') ('8', 'p') ('万', 'w') ('员工', 'n') ('，', 'w') ('业务', 'mq') ('遍及', 'v') ('170多个', 'mq') ('国', 'n') ('家', 'p') ('和', 'cc') ('地区', 'n') ('。', 'w')\n"]}],"source":["segmenter: DLSegmenter = get_or_create(\"./tookit/bi-lstm-crf-master/config/default-config.json\",\n","                                       src_dict_path=\"./tookit/bi-lstm-crf-master/config/src_dict.json\",\n","                                       tgt_dict_path=\"./tookit/bi-lstm-crf-master/config/tgt_dict.json\",\n","                                       weights_path=\"./tookit/bi-lstm-crf-master/models/weights.32--0.18.h5\")\n","texts = [ \n","   \"华为是全球领先的ICT（信息与通信）基础设施和智能终端提供商，\"\n","    \"致力于把数字世界带入每个人、每个家庭、每个组织，构建万物互联的智能世界。\"\n","    \"我们在通信网络、IT、智能终端和云服务等领域为客户提供有竞争力、安全可信赖的产品、解决方案与服务，\"\n","    \"与生态伙伴开放合作，持续为客户创造价值，释放个人潜能，丰富家庭生活，激发组织创新。\"\n","    \"华为坚持围绕客户需求持续创新，加大基础研究投入，厚积薄发，推动世界进步。\"\n","    \"华为成立于1987年，是一家由员工持有全部股份的民营企业，目前有18万员工，业务遍及170多个国家和地区。\"       \n","]\n","\n","\n","for sent, tag in segmenter.decode_texts(texts):\n","    print(*zip(sent,tag))\n","\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":2}
